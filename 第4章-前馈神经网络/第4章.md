# 4-1 

> 对于一个神经元$\sigma(\mathbf w ^\mathsf T \mathbf x +b)$，并使用梯度下降优化参数$\mathbf w$，如果输入$\mathbf x$恒大于0，其收敛速度会比零均值化的输入更慢



假设这次采用sigmoid作为激活函数
$$
\begin{align}
\frac{\partial{L}}{\partial{w}}&=\frac{\partial{L}}{\partial{a}}\cdot\frac{\partial{a}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{w}}\\
&=\frac{\partial{L}}{\partial{a}}\cdot\sigma{(wx)}\cdot(1-\sigma{(wx)})\cdot x
\end{align}
$$
由上式可知，当$x$恒大于零时，梯度的符号仅与$\frac{\partial{L}}{\partial{a}}$有关，所以，各个权重都会向同一个方向更新，造成z型更新的情况

![image-20201019161626029](第4章.assets/image-20201019161626029.png)

以只有两维的$w_1$和$w_2$做例子，因为$w_1$和$w_2$的梯度符号一致，要么都为正要么都为负，都为正时则往第一象限变化，都为负时则往第三象限变化，而如果此时最优解在第四象限，则实际训练路径会远远长于最短路径，就会使得网络收敛速度变慢。

所以为了避免Z型更新的情况，将输入进行零均值化，这样某次输入全部为正的可能性就很小，就会是正负掺杂的输入，每个$w$的梯度符号和输入$x$相关，也就不会全部都一样，也就不会Z型更新了。

# 4-2

> 试设计一个前馈神经网络来解决XOR问题，要求该前馈神经网络具有两个隐藏神经元和一个输出神经元，并使用ReLU作为激活函数

![image-20201019130548874](第4章.assets/image-20201019130548874.png)

![image-20201019130608650](第4章.assets/image-20201019130608650.png)

# 4-3

> 试举例说明“死亡ReLU问题”，并提出解决方法



举例：

![image-20201019134002228](第4章.assets/image-20201019134002228.png)

对上面的网络结构，$W$是$2\times 4$的矩阵，输入是$4\times 1$的向量，第二层的激活函数为ReLU
$$
z_1 =
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14}
\end{bmatrix}\cdot
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}
$$
如果由于上一次参数更新时，梯度过大，或者学习率设置太大，导致权重一下子更新过多。

就会导致

$z_1 < 0$

$a_1 = ReLU(z_1)=max(z_1)=0$

即激活函数的输出为0

在此次的梯度更新时：

$\frac{\partial{L}}{\partial{z_1}}=\frac{\partial{L}}{\partial{a_1}}\cdot\frac{\partial{a_1}}{\partial{z_1}}=\frac{\partial{L}}{\partial{a_1}}\cdot 0=0$

而

$\frac{\partial{L}}{\partial{W_1}} = \frac{\partial{L}}{\partial{z_1}}\cdot x^\mathsf T = \vec{0}$

也就是说，这几个参数将不再被更新了



解决方法：

1. 把 ReLU换成LeakyReLU ，保证让激活函数在输入小于零的情况下也有非零的输出。
2. 采用较小的学习率
3. 采用 momentum based 优化算法，动态调整学习率